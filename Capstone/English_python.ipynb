{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Cornell_Dialog_Encoder_Decoder (7).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c043e7683e51402181560015300eff51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_553c55b6e1ca480eafe84046002ff10d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a84167903f0a4dcd95e1836d20da1794",
              "IPY_MODEL_4a35efc0734b4443ae139ef0a530f8a9"
            ]
          }
        },
        "553c55b6e1ca480eafe84046002ff10d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a84167903f0a4dcd95e1836d20da1794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_901e4a7ace4943558560ff7957adb58b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_831c570d5ddd4616a385728d602786bb"
          }
        },
        "4a35efc0734b4443ae139ef0a530f8a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3263843a94fb40f1b818e9cb09f784f4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/? [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4d3e1c9492b44c64ad4d99522900e00a"
          }
        },
        "901e4a7ace4943558560ff7957adb58b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "831c570d5ddd4616a385728d602786bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3263843a94fb40f1b818e9cb09f784f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4d3e1c9492b44c64ad4d99522900e00a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88afaa7293f846afa165e42376504de1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_988fc26eb1804fa8a9f18af2af51a30f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c06e9aa1a0974892896321c5c3b7f04c",
              "IPY_MODEL_0d8e42e1ebb04988a1a8733c88187d01"
            ]
          }
        },
        "988fc26eb1804fa8a9f18af2af51a30f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c06e9aa1a0974892896321c5c3b7f04c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f0169483af3244a9a2ccf756967c1c36",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4727,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4727,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8670fbf370b74c37b9011b0f85d5d9a9"
          }
        },
        "0d8e42e1ebb04988a1a8733c88187d01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_59c3b67a7f254e7dabf3ca75c576b379",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4727/4727 [00:21&lt;00:00, 221.13it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a98cefb0e0cb4b73b886f31e431c936a"
          }
        },
        "f0169483af3244a9a2ccf756967c1c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8670fbf370b74c37b9011b0f85d5d9a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "59c3b67a7f254e7dabf3ca75c576b379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a98cefb0e0cb4b73b886f31e431c936a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e007ea4a481541cc8d2b04992faca852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_024ce9a037d44e4d876cdbaecefef5ec",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_67e102254952494f8d65a08c059f94cb",
              "IPY_MODEL_c81482a1b0684c609c83e527a6f6dd5d"
            ]
          }
        },
        "024ce9a037d44e4d876cdbaecefef5ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67e102254952494f8d65a08c059f94cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_26e48e0eb37c4a3281d9d212257ce3f7",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4727,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4727,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d78c2ebfbbeb4015b5204f040fcda460"
          }
        },
        "c81482a1b0684c609c83e527a6f6dd5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e5f9585c2659425bbc63aa7032c4b6ee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4727/4727 [00:15&lt;00:00, 295.76it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_271aee46e06b4f02a8685c0fb6afb16d"
          }
        },
        "26e48e0eb37c4a3281d9d212257ce3f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d78c2ebfbbeb4015b5204f040fcda460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e5f9585c2659425bbc63aa7032c4b6ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "271aee46e06b4f02a8685c0fb6afb16d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabhkirar/NLP_END/blob/main/Capstone/English_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzxOJwN7EzFO"
      },
      "source": [
        "# 2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
        "\n",
        "In this second notebook on sequence-to-sequence models using PyTorch and TorchText, we'll be implementing the model from [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078). This model will achieve improved test perplexity whilst only using a single layer RNN in both the encoder and the decoder.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Let's remind ourselves of the general encoder-decoder model.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq1.png?raw=1)\n",
        "\n",
        "We use our encoder (green) over the embedded source sequence (yellow) to create a context vector (red). We then use that context vector with the decoder (blue) and a linear layer (purple) to generate the target sentence.\n",
        "\n",
        "In the previous model, we used an multi-layered LSTM as the encoder and decoder.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq4.png?raw=1)\n",
        "\n",
        "One downside of the previous model is that the decoder is trying to cram lots of information into the hidden states. Whilst decoding, the hidden state will need to contain information about the whole of the source sequence, as well as all of the tokens have been decoded so far. By alleviating some of this information compression, we can create a better model!\n",
        "\n",
        "We'll also be using a GRU (Gated Recurrent Unit) instead of an LSTM (Long Short-Term Memory). Why? Mainly because that's what they did in the paper (this paper also introduced GRUs) and also because we used LSTMs last time. To understand how GRUs (and LSTMs) differ from standard RNNS, check out [this](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) link. Is a GRU better than an LSTM? [Research](https://arxiv.org/abs/1412.3555) has shown they're pretty much the same, and both are better than standard RNNs. \n",
        "\n",
        "## Preparing Data\n",
        "\n",
        "All of the data preparation will be (almost) the same as last time, so we'll very briefly detail what each code block does. See the previous notebook for a recap.\n",
        "\n",
        "We'll import PyTorch, TorchText, spaCy and a few standard modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAOn6GYN2cuQ",
        "outputId": "db331113-7b7a-4d54-8b48-8edd55430f59"
      },
      "source": [
        "!pip install -U torch==1.7.0\n",
        "!pip install -U torchtext==0.8.1\n",
        "!pip install -U torchvision==0.8.0\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.7.0\n",
            "  Using cached https://files.pythonhosted.org/packages/d9/74/d52c014fbfb50aefc084d2bf5ffaa0a8456f69c586782b59f93ef45e2da9/torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (0.6)\n",
            "\u001b[31mERROR: torchtext 0.8.1 has requirement torch==1.7.1, but you'll have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.7.1\n",
            "    Uninstalling torch-1.7.1:\n",
            "      Successfully uninstalled torch-1.7.1\n",
            "Successfully installed torch-1.7.0\n",
            "Requirement already up-to-date: torchtext==0.8.1 in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (1.19.5)\n",
            "Collecting torch==1.7.1\n",
            "  Using cached https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->torchtext==0.8.1) (3.7.4.3)\n",
            "\u001b[31mERROR: torchvision 0.8.0 has requirement torch==1.7.0, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.7.0\n",
            "    Uninstalling torch-1.7.0:\n",
            "      Successfully uninstalled torch-1.7.0\n",
            "Successfully installed torch-1.7.1\n",
            "Requirement already up-to-date: torchvision==0.8.0 in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Collecting torch==1.7.0\n",
            "  Using cached https://files.pythonhosted.org/packages/d9/74/d52c014fbfb50aefc084d2bf5ffaa0a8456f69c586782b59f93ef45e2da9/torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.0) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.0) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision==0.8.0) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision==0.8.0) (0.6)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision==0.8.0) (0.16.0)\n",
            "\u001b[31mERROR: torchtext 0.8.1 has requirement torch==1.7.1, but you'll have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.7.1\n",
            "    Uninstalling torch-1.7.1:\n",
            "      Successfully uninstalled torch-1.7.1\n",
            "Successfully installed torch-1.7.0\n",
            "Requirement already up-to-date: torch==1.7.0 in /usr/local/lib/python3.7/dist-packages (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (0.6)\n",
            "Requirement already up-to-date: torchtext==0.8.1 in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (2.23.0)\n",
            "Collecting torch==1.7.1\n",
            "  Using cached https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->torchtext==0.8.1) (3.7.4.3)\n",
            "\u001b[31mERROR: torchvision 0.8.0 has requirement torch==1.7.0, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.7.0\n",
            "    Uninstalling torch-1.7.0:\n",
            "      Successfully uninstalled torch-1.7.0\n",
            "Successfully installed torch-1.7.1\n",
            "Requirement already up-to-date: torchvision==0.8.0 in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Collecting torch==1.7.0\n",
            "  Using cached https://files.pythonhosted.org/packages/d9/74/d52c014fbfb50aefc084d2bf5ffaa0a8456f69c586782b59f93ef45e2da9/torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.0) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.0) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision==0.8.0) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision==0.8.0) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision==0.8.0) (0.6)\n",
            "\u001b[31mERROR: torchtext 0.8.1 has requirement torch==1.7.1, but you'll have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.7.1\n",
            "    Uninstalling torch-1.7.1:\n",
            "      Successfully uninstalled torch-1.7.1\n",
            "Successfully installed torch-1.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUnMLdevEzFT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.data import Field\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srzErHAsEzFU"
      },
      "source": [
        "Then set a random seed for deterministic results/reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_wP4J4LEzFX"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hawianoFEzFY"
      },
      "source": [
        "Instantiate our German and English spaCy models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BP3YSvJEzFY",
        "outputId": "0bbfd20a-293f-4dff-b224-ac2a3f6a584b"
      },
      "source": [
        "%%bash\n",
        "python -m spacy download en\n",
        "python -m spacy download de\n",
        "#spacy_de = spacy.load('de')\n",
        "#spacy_en = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2mâœ” Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2mâœ” Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAq6OVJKfXm3"
      },
      "source": [
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bEkyPt5EzFY"
      },
      "source": [
        "Previously we reversed the source (German) sentence, however in the paper we are implementing they don't do this, so neither will we."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KaGEZ45EzFZ"
      },
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM34DCqNURwX",
        "outputId": "7ca5b7e4-7bc5-403a-ccc0-1376b429db8e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTGgcuylUiKH",
        "outputId": "ac3f6a35-81af-4ec4-b405-550d3e77b0fe"
      },
      "source": [
        "!ls '/content/gdrive/MyDrive/data'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cornell  cornell_movie_dialogs_corpus.zip  english-python.pt  python_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V9qZho8UnAT"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-MHoWNwUtF0",
        "outputId": "b8c7a41e-3c37-4e38-ceb0-554b98fc000f"
      },
      "source": [
        "corpus_name = \"data\"\n",
        "corpus = os.path.join(\"/content/gdrive/MyDrive\", corpus_name)\n",
        "\n",
        "def printLines(file, n=10):\n",
        "    print(file)\n",
        "    with open(file, 'rb') as datafile:\n",
        "        lines = datafile.readlines()\n",
        "    for line in lines[:n]:\n",
        "        print(line)\n",
        "\n",
        "printLines(os.path.join(corpus, \"python_data.txt\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/data/python_data.txt\n",
            "b'# write a python program to add two numbers \\r\\n'\n",
            "b'num1 = 1.5\\r\\n'\n",
            "b'num2 = 6.3\\r\\n'\n",
            "b'sum = num1 + num2\\r\\n'\n",
            "b\"print(f'Sum: {sum}')\\r\\n\"\n",
            "b'\\r\\n'\n",
            "b'# write a python function to add two user provided numbers and return the sum\\r\\n'\n",
            "b'def add_two_numbers(num1, num2):\\r\\n'\n",
            "b'    sum = num1 + num2\\r\\n'\n",
            "b'    return sum\\r\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKXV-WgWNCx8"
      },
      "source": [
        "english_text_python_program_pair_list = []\n",
        "process_python_code=False\n",
        "i=1\n",
        "with open('/content/gdrive/MyDrive/data/python_data.txt', 'r', encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        #print(i)\n",
        "        i += 1\n",
        "        if process_python_code==False:\n",
        "            if line.strip() == '':\n",
        "                continue\n",
        "            if line.startswith('#'):\n",
        "                english_text = line\n",
        "                #english_text_list.append(line)\n",
        "                process_python_code=True\n",
        "                python_program=''\n",
        "            else:\n",
        "                print(i, \": \", line)            \n",
        "        else:\n",
        "            if line.strip() == '':\n",
        "                process_python_code=False\n",
        "                english_text_python_program_pair_list.append((english_text, python_program))\n",
        "                python_program=''\n",
        "                english_text =''\n",
        "            if line.lstrip().startswith('#'):\n",
        "                continue\n",
        "            else:\n",
        "                python_program += line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls-5mZrkNfzN",
        "outputId": "a8cd3470-8a83-441e-9665-6b3ea07c3c64"
      },
      "source": [
        "len(english_text_python_program_pair_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4727"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoDwg8N3AxBM"
      },
      "source": [
        "english_text_list,python_program_list  = zip(*english_text_python_program_pair_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGQ_PNHLAydR"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'English': english_text_list, 'Python':python_program_list })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "HSSq3thYA276",
        "outputId": "e6aec205-a44d-464b-fcd2-12ca73e392eb"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Python</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td># write a python program to add two numbers \\n</td>\n",
              "      <td>num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td># write a python function to add two user prov...</td>\n",
              "      <td>def add_two_numbers(num1, num2):\\n    sum = nu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td># write a program to find and print the larges...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &gt;= n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td># write a program to find and print the smalle...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &lt;= n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td># Write a python function to merge two given l...</td>\n",
              "      <td>def merge_lists(l1, l2):\\n    return l1 + l2\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td># Write a program to check whether a number is...</td>\n",
              "      <td>num = 337\\nif num &gt; 1:\\n   for i in range(2, n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td># Write a python function that prints the fact...</td>\n",
              "      <td>def print_factors(x):\\n   print(f\"The factors ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td># Write a program to find the factorial of a n...</td>\n",
              "      <td>num = 13\\nfactorial = 1\\nif num &lt; 0:\\n   print...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td># Write a python function to print whether a n...</td>\n",
              "      <td>def check_pnz(num):\\n    if num &gt; 0:\\n       p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td># Write a program to print the multiplication ...</td>\n",
              "      <td>num = 9\\nfor i in range(1, 11):\\n   print(f\"{n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             English                                             Python\n",
              "0     # write a python program to add two numbers \\n  num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...\n",
              "1  # write a python function to add two user prov...  def add_two_numbers(num1, num2):\\n    sum = nu...\n",
              "2  # write a program to find and print the larges...  num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 >= n...\n",
              "3  # write a program to find and print the smalle...  num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 <= n...\n",
              "4  # Write a python function to merge two given l...     def merge_lists(l1, l2):\\n    return l1 + l2\\n\n",
              "5  # Write a program to check whether a number is...  num = 337\\nif num > 1:\\n   for i in range(2, n...\n",
              "6  # Write a python function that prints the fact...  def print_factors(x):\\n   print(f\"The factors ...\n",
              "7  # Write a program to find the factorial of a n...  num = 13\\nfactorial = 1\\nif num < 0:\\n   print...\n",
              "8  # Write a python function to print whether a n...  def check_pnz(num):\\n    if num > 0:\\n       p...\n",
              "9  # Write a program to print the multiplication ...  num = 9\\nfor i in range(1, 11):\\n   print(f\"{n..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qc0oQkmBa_-",
        "outputId": "29999d45-de80-47f1-8d9e-6ca51d693c37"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "import random\n",
        "import random\n",
        "#import google_trans_new\n",
        "#from google_trans_new import google_translator\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "lem = WordNetLemmatizer()\n",
        "#translator = google_translator()\n",
        "\n",
        "def clean_text(text):\n",
        "    ## lower case\n",
        "    if not isinstance(text, str):\n",
        "      return str(text) \n",
        "    cleaned = text.lower()\n",
        "\n",
        "    urls_pattern = re.compile(r'https?://\\S+|www.\\S+')\n",
        "    cleaned = urls_pattern.sub(r'',cleaned)\n",
        "    \n",
        "    ## remove punctuations\n",
        "    punctuations = string.punctuation\n",
        "    cleaned_temp = \"\".join(character for character in cleaned if character not in punctuations)\n",
        "    \n",
        "    ## remove stopwords \n",
        "    words = cleaned_temp.split()\n",
        "    #stopword_lists = stopwords.words(\"english\")\n",
        "    #cleaned = [word for word in words if word not in stopword_lists]\n",
        "    cleaned = words\n",
        "    \n",
        "    ## normalization - lemmatization\n",
        "    #cleaned = [lem.lemmatize(word, \"v\") for word in cleaned]\n",
        "    #cleaned = [lem.lemmatize(word, \"n\") for word in cleaned]\n",
        "    \n",
        "    ## join \n",
        "    cleaned = \" \".join(cleaned)\n",
        "    return cleaned\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "c043e7683e51402181560015300eff51",
            "553c55b6e1ca480eafe84046002ff10d",
            "a84167903f0a4dcd95e1836d20da1794",
            "4a35efc0734b4443ae139ef0a530f8a9",
            "901e4a7ace4943558560ff7957adb58b",
            "831c570d5ddd4616a385728d602786bb",
            "3263843a94fb40f1b818e9cb09f784f4",
            "4d3e1c9492b44c64ad4d99522900e00a",
            "88afaa7293f846afa165e42376504de1",
            "988fc26eb1804fa8a9f18af2af51a30f",
            "c06e9aa1a0974892896321c5c3b7f04c",
            "0d8e42e1ebb04988a1a8733c88187d01",
            "f0169483af3244a9a2ccf756967c1c36",
            "8670fbf370b74c37b9011b0f85d5d9a9",
            "59c3b67a7f254e7dabf3ca75c576b379",
            "a98cefb0e0cb4b73b886f31e431c936a"
          ]
        },
        "id": "LJnuVpb-BTFG",
        "outputId": "287aa654-9c24-45b7-a958-845c09b5bfa7"
      },
      "source": [
        "from tqdm import tqdm_notebook as tqdm\n",
        "tqdm().pandas() \n",
        "\n",
        "df['English'] = df['English'].progress_apply(lambda txt: txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c043e7683e51402181560015300eff51",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88afaa7293f846afa165e42376504de1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4727.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "e007ea4a481541cc8d2b04992faca852",
            "024ce9a037d44e4d876cdbaecefef5ec",
            "67e102254952494f8d65a08c059f94cb",
            "c81482a1b0684c609c83e527a6f6dd5d",
            "26e48e0eb37c4a3281d9d212257ce3f7",
            "d78c2ebfbbeb4015b5204f040fcda460",
            "e5f9585c2659425bbc63aa7032c4b6ee",
            "271aee46e06b4f02a8685c0fb6afb16d"
          ]
        },
        "id": "4tvulIotBvPZ",
        "outputId": "0c7c8e5a-b80d-4c59-ec4c-8ccad4d4041e"
      },
      "source": [
        "df['Python'] = df['Python'].progress_apply(lambda txt: txt.lstrip())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e007ea4a481541cc8d2b04992faca852",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4727.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "JKEeJH2qCA_t",
        "outputId": "1db1bfd8-1651-4d2b-8bc4-4da39898621a"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Python</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td># write a python program to add two numbers \\n</td>\n",
              "      <td>num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td># write a python function to add two user prov...</td>\n",
              "      <td>def add_two_numbers(num1, num2):\\n    sum = nu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td># write a program to find and print the larges...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &gt;= n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td># write a program to find and print the smalle...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &lt;= n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td># Write a python function to merge two given l...</td>\n",
              "      <td>def merge_lists(l1, l2):\\n    return l1 + l2\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td># Write a program to check whether a number is...</td>\n",
              "      <td>num = 337\\nif num &gt; 1:\\n   for i in range(2, n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td># Write a python function that prints the fact...</td>\n",
              "      <td>def print_factors(x):\\n   print(f\"The factors ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td># Write a program to find the factorial of a n...</td>\n",
              "      <td>num = 13\\nfactorial = 1\\nif num &lt; 0:\\n   print...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td># Write a python function to print whether a n...</td>\n",
              "      <td>def check_pnz(num):\\n    if num &gt; 0:\\n       p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td># Write a program to print the multiplication ...</td>\n",
              "      <td>num = 9\\nfor i in range(1, 11):\\n   print(f\"{n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             English                                             Python\n",
              "0     # write a python program to add two numbers \\n  num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...\n",
              "1  # write a python function to add two user prov...  def add_two_numbers(num1, num2):\\n    sum = nu...\n",
              "2  # write a program to find and print the larges...  num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 >= n...\n",
              "3  # write a program to find and print the smalle...  num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 <= n...\n",
              "4  # Write a python function to merge two given l...     def merge_lists(l1, l2):\\n    return l1 + l2\\n\n",
              "5  # Write a program to check whether a number is...  num = 337\\nif num > 1:\\n   for i in range(2, n...\n",
              "6  # Write a python function that prints the fact...  def print_factors(x):\\n   print(f\"The factors ...\n",
              "7  # Write a program to find the factorial of a n...  num = 13\\nfactorial = 1\\nif num < 0:\\n   print...\n",
              "8  # Write a python function to print whether a n...  def check_pnz(num):\\n    if num > 0:\\n       p...\n",
              "9  # Write a program to print the multiplication ...  num = 9\\nfor i in range(1, 11):\\n   print(f\"{n..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1U6dt9CHpSS",
        "outputId": "e0cfcfd8-4039-41d1-ffbd-b39a4bfe02a4"
      },
      "source": [
        "SRC = Field(tokenize=tokenize_en, \n",
        "            init_token='<sos>', \n",
        "            eos_token='<eos>',            \n",
        "            batch_first = True, \n",
        "            lower=True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token='<sos>', \n",
        "            eos_token='<eos>', \n",
        "            batch_first = True\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9Cq9YpFCF14"
      },
      "source": [
        "fields = [('English', SRC),('Python',TRG)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNpkuJq7HoUg"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, valid = train_test_split(df, test_size=0.02)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jES070ThIBNq"
      },
      "source": [
        "train = train.reset_index(drop=True) ## This is being done because data.Example.fromlist was failing\n",
        "valid = valid.reset_index(drop=True) ## This is being done because data.Example.fromlist was failing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1ueu0PaIEma"
      },
      "source": [
        "MAX_OUTPUT_SEQ_LENGTH = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvn85iFZIUzt"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.data import Field, BucketIterator, Example, Dataset\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qtkkMweIKY9",
        "outputId": "23c0819d-bcab-4375-91d7-96cdda9a3dbd"
      },
      "source": [
        "example_trng = [Example.fromlist([train.English[i],train.Python[i]], fields) for i in range(train.shape[0]) if len(tokenize_en(train.Python[i])) <= MAX_OUTPUT_SEQ_LENGTH - 4 ] \n",
        "example_val = [Example.fromlist([valid.English[i],valid.Python[i]], fields) for i in range(valid.shape[0]) if len(tokenize_en(valid.Python[i])) <= MAX_OUTPUT_SEQ_LENGTH - 4 ] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXAK2LlqIdsl"
      },
      "source": [
        "train_dataset = Dataset(example_trng, fields)\n",
        "valid_dataset = Dataset(example_val, fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKgtTU38Io2s",
        "outputId": "250b97dd-c0e1-4472-a569-79be76c89792"
      },
      "source": [
        "vars(train_dataset.examples[10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'English': ['#', 'convert', 'string', 'into', 'a', 'datetime', 'object'],\n",
              " 'Python': ['from',\n",
              "  'datetime',\n",
              "  'import',\n",
              "  'datetime',\n",
              "  '\\n',\n",
              "  'date_string',\n",
              "  '=',\n",
              "  '\"',\n",
              "  'Feb',\n",
              "  '25',\n",
              "  '2020',\n",
              "  ' ',\n",
              "  '4:20PM',\n",
              "  '\"',\n",
              "  '\\n',\n",
              "  'datetime_object',\n",
              "  '=',\n",
              "  'datetime.strptime(date_string',\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '%',\n",
              "  'b',\n",
              "  '%',\n",
              "  'd',\n",
              "  '%',\n",
              "  'Y',\n",
              "  '%',\n",
              "  'I:%M%p',\n",
              "  \"'\",\n",
              "  ')',\n",
              "  '\\n',\n",
              "  'print(datetime_object',\n",
              "  ')']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGNIQV0-Ir_3",
        "outputId": "96421b69-55e6-4d5b-e71e-6822caf0940a"
      },
      "source": [
        "vars(valid_dataset.examples[10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'English': ['#',\n",
              "  'attach',\n",
              "  'function',\n",
              "  'closure',\n",
              "  'with',\n",
              "  'logs',\n",
              "  'details',\n",
              "  'to',\n",
              "  'another',\n",
              "  'function'],\n",
              " 'Python': ['def',\n",
              "  'attach_log(fn',\n",
              "  ':',\n",
              "  '\"',\n",
              "  'function',\n",
              "  '\"',\n",
              "  ')',\n",
              "  ':',\n",
              "  '\\n    ',\n",
              "  'def',\n",
              "  'inner(*args',\n",
              "  ',',\n",
              "  '*',\n",
              "  '*',\n",
              "  'kwargs',\n",
              "  ')',\n",
              "  ':',\n",
              "  '\\n        ',\n",
              "  'dt',\n",
              "  '=',\n",
              "  'datetime.now',\n",
              "  '(',\n",
              "  ')',\n",
              "  '\\n        ',\n",
              "  \"print(f'{fn.__name\",\n",
              "  '_',\n",
              "  '_',\n",
              "  '}',\n",
              "  'is',\n",
              "  'called',\n",
              "  'at',\n",
              "  '{',\n",
              "  'dt',\n",
              "  '}',\n",
              "  'with',\n",
              "  '{',\n",
              "  'args',\n",
              "  '}',\n",
              "  '{',\n",
              "  'kwargs',\n",
              "  '}',\n",
              "  \"'\",\n",
              "  ')',\n",
              "  '\\n        ',\n",
              "  'return',\n",
              "  'fn(*args',\n",
              "  ',',\n",
              "  '*',\n",
              "  '*',\n",
              "  'kwargs',\n",
              "  ')',\n",
              "  '\\n    ',\n",
              "  'return',\n",
              "  'inner']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUd2-nQ6I220"
      },
      "source": [
        "SRC.build_vocab(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJI4EpB1I4NG"
      },
      "source": [
        "TRG.build_vocab(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dmsx1WvEtwE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fhx__qZF3Hx"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9Y7dJieF7CI",
        "outputId": "027cc2ec-c0ca-4e6e-e548-3a43202b4c7c"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator = BucketIterator.splits(\n",
        "    (train_dataset, valid_dataset), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key=lambda x:len(x.English),\n",
        "    sort_within_batch = False, \n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-symyN1zV-_q"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jarF42K0WNOX"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqJrb9iLWUHK"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqfGHRX9WZZf"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "        #pos = [batch size, trg len]\n",
        "            \n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv8nGIYIWHgY"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVmuR1X_Wgi4"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        # query, key, value\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIjpYdO4Wm7s"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVnrgIhbWtaG"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yESIrDzr0mNi",
        "outputId": "7ef765b8-effa-4883-883c-65e997856c37"
      },
      "source": [
        "print(OUTPUT_DIM)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEpPZIpK2-5x",
        "outputId": "6bcacbc9-e987-4758-82fc-bb0cda3bf01a"
      },
      "source": [
        "print(INPUT_DIM)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0KBqQ2BWxuk"
      },
      "source": [
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsZC-bHkW3CT",
        "outputId": "50abc2e5-9961-4c5d-a673-b0e9f58c0a10"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 8,892,237 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3koKqh6GW7m4"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKj7voEHXArd"
      },
      "source": [
        "model.apply(initialize_weights);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juEcz_WrXFeY"
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnrl6DgqXJet"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5HKu_FFXOQp"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.English\n",
        "        trg = batch.Python\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        print(trg.shape)\n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6xj2J-tXT8B"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.English\n",
        "            trg = batch.Python\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            SRC.build_vocab(train_dataset)\n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0idGzciCXae0"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJiU-8agXet8",
        "outputId": "74d50609-b09f-49f4-cd34-426e970df71e"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 94])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 93])\n",
            "torch.Size([128, 91])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 88])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 89])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 88])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([102, 92])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 88])\n",
            "torch.Size([128, 97])\n",
            "Epoch: 01 | Time: 5m 44s\n",
            "\tTrain Loss: 6.748 | Train PPL: 852.007\n",
            "\t Val. Loss: 5.207 |  Val. PPL: 182.504\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 89])\n",
            "torch.Size([128, 91])\n",
            "torch.Size([128, 87])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 88])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 91])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([102, 97])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 89])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 91])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 97])\n",
            "Epoch: 02 | Time: 5m 40s\n",
            "\tTrain Loss: 4.660 | Train PPL: 105.626\n",
            "\t Val. Loss: 4.008 |  Val. PPL:  55.060\n",
            "torch.Size([128, 93])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 89])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 91])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 85])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 85])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([102, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 91])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 85])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 97])\n",
            "Epoch: 03 | Time: 5m 37s\n",
            "\tTrain Loss: 3.636 | Train PPL:  37.939\n",
            "\t Val. Loss: 3.378 |  Val. PPL:  29.326\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 86])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 89])\n",
            "torch.Size([128, 93])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 85])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 88])\n",
            "torch.Size([128, 76])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([102, 88])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 93])\n",
            "torch.Size([128, 89])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 88])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 91])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 87])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 86])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 96])\n",
            "Epoch: 04 | Time: 5m 41s\n",
            "\tTrain Loss: 3.080 | Train PPL:  21.761\n",
            "\t Val. Loss: 3.010 |  Val. PPL:  20.288\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 89])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([102, 92])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 88])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 89])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 92])\n",
            "Epoch: 05 | Time: 5m 48s\n",
            "\tTrain Loss: 2.692 | Train PPL:  14.759\n",
            "\t Val. Loss: 2.724 |  Val. PPL:  15.234\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 90])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 93])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 88])\n",
            "torch.Size([128, 89])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 93])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 93])\n",
            "torch.Size([102, 90])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 95])\n",
            "Epoch: 06 | Time: 5m 41s\n",
            "\tTrain Loss: 2.384 | Train PPL:  10.853\n",
            "\t Val. Loss: 2.473 |  Val. PPL:  11.863\n",
            "torch.Size([128, 91])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 89])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 87])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 86])\n",
            "torch.Size([128, 93])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 83])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([102, 96])\n",
            "Epoch: 07 | Time: 5m 48s\n",
            "\tTrain Loss: 2.122 | Train PPL:   8.351\n",
            "\t Val. Loss: 2.305 |  Val. PPL:  10.020\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 91])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 88])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 89])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([102, 85])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 87])\n",
            "torch.Size([128, 91])\n",
            "torch.Size([128, 91])\n",
            "torch.Size([128, 87])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 88])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 95])\n",
            "Epoch: 08 | Time: 5m 39s\n",
            "\tTrain Loss: 1.903 | Train PPL:   6.704\n",
            "\t Val. Loss: 2.152 |  Val. PPL:   8.601\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 93])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 89])\n",
            "torch.Size([128, 90])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 91])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 86])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 87])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([102, 95])\n",
            "torch.Size([128, 90])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 95])\n",
            "Epoch: 09 | Time: 5m 49s\n",
            "\tTrain Loss: 1.706 | Train PPL:   5.504\n",
            "\t Val. Loss: 1.999 |  Val. PPL:   7.378\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 92])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 87])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 91])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 93])\n",
            "torch.Size([102, 95])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 91])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 97])\n",
            "torch.Size([128, 94])\n",
            "torch.Size([128, 86])\n",
            "torch.Size([128, 95])\n",
            "torch.Size([128, 96])\n",
            "torch.Size([128, 93])\n",
            "torch.Size([128, 85])\n",
            "Epoch: 10 | Time: 5m 41s\n",
            "\tTrain Loss: 1.525 | Train PPL:   4.596\n",
            "\t Val. Loss: 1.883 |  Val. PPL:   6.575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yTEeDuReBLK",
        "outputId": "58a98530-e339-481f-80f0-af18bfe5b5b1"
      },
      "source": [
        "model.load_state_dict(torch.load('tut6-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 1.883 | Test PPL:   6.575 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLufM2mGeOtp"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "    \n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('en')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    \n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqa7KbWyeme4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb2db46-ceb0-45ee-9cd7-04dd9a3dc02b"
      },
      "source": [
        "example_idx = 8\n",
        "\n",
        "src = vars(train_dataset.examples[example_idx])['English']\n",
        "trg = vars(train_dataset.examples[example_idx])['Python']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['#', 'write', 'a', 'python', 'program', 'to', 'find', 'the', 'total', 'number', 'of', 'letters', 'and', 'digits', 'in', 'a', 'given', 'string']\n",
            "trg = [\"str1='TestStringwith123456789\", \"'\", '\\n', 'no_of_letters', ',', 'no_of_digits', '=', '0,0', '\\n', 'for', 'c', 'in', 'str1', ':', '\\n  ', 'no_of_letters', '+', '=', 'c.isalpha', '(', ')', '\\n  ', 'no_of_digits', '+', '=', 'c.isnumeric', '(', ')', '\\n', 'print(no_of_letters', ')', '\\n', 'print(no_of_digits', ')']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8lVDbmXfQfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94eacefe-fc37-4952-ca93-d88d68a90bc2"
      },
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['str1', '=', '\"', 'abc4234AFde', '\"', '\\n', 'digitCount', '=', '0', '\\n', 'for', 'i', 'in', 'range(0,len(str1', ')', ')', ')', ':', '\\n  ', 'char', '=', 'str1[i', ']', '\\n  ', 'char', '=', 'str1[i', ']', '\\n  ', 'if(char.isdigit', '(', ')', ')', ':', '\\n    ', 'digitCount', '+', '=', '1', '\\n', \"print('Number\", 'of', 'digits', ':', \"'\", ',', 'digitCount', ')', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlpVqymUgHLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c44727-f4b0-4b29-a32f-835a6596ca80"
      },
      "source": [
        "example_idx = 0\n",
        "\n",
        "src = vars(train_dataset.examples[example_idx])['English']\n",
        "trg = vars(train_dataset.examples[example_idx])['Python']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['#', 'write', 'a', 'python', 'function', 'that', 'takes', 'a', 'list', 'as', 'an', 'input', 'and', 'converts', 'all', 'numbers', 'to', 'negative', 'numbers', 'and', 'returns', 'the', 'new', 'list']\n",
            "trg = ['def', 'make_all_negative(nums', ')', ':', '\\n   ', 'return', '[', 'num', 'if', 'num', '<', '0', 'else', '-num', 'for', 'num', 'in', 'nums', ']']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcgS8xRAgMOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd8823b4-382c-43e3-e280-e568112c605c"
      },
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['def', 'printList', '(', ')', ':', '\\n    ', 'list1', '=', 'list', '(', ')', '\\n    ', 'for', 'i', 'in', 'range(1', ',', 'l2', ':', '\\n        ', 'if', 'num', '%', '2', '=', '0', ')', ':', '\\n            ', 'yield', 'list', '(', ')', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY7SsC8TEzFn"
      },
      "source": [
        "Just looking at the test loss, we get better performance. This is a pretty good sign that this model architecture is doing something right! Relieving the information compression seems like the way forard, and in the next tutorial we'll expand on this even further with *attention*."
      ]
    }
  ]
}